# 3D Vision and Language Research ✨
This is a curated list of "3D Vision and Language" research which is maintained by [haonan](https://github.com/zchoi). Watch this repository for the latest updates!

## 🍃Table of Contents
- [3D Multimodal Pretraining](#3D-multimodal-pretraining)
- [3D Visual Understanding](#3D-visual-understanding)
- [3D Dense Captioning](#3D-dense-captioning)
- [3D Cross-Modal Retrieval](#3D-cross-modal-retrieval)
- [3D Visual QA](#3D-visual-qa)
- [3D Visual Grounding](#3D-visual-grounding)

## Methods

> ### 3D Multimodal Pretraining

* [**Context-aware Alignment and Mutual Masking for 3D-Language Pre-training**](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf) [**CVPR 2023**] <br>
Zhao Jin<sup>1</sup>, Munawar Hayat<sup>2</sup>, Yuwei Yang<sup>1</sup>, Yulan Guo<sup>3</sup>, Yinjie Lei<sup>1†</sup><br>
<sup>1</sup>Sichuan University, <sup>2</sup>Monash University, <sup>3</sup>Sun Yat-sen University


> ### 3D Visual Understanding

* [**ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding**](https://arxiv.org/pdf/2212.05171.pdf) [**CVPR 2023**] <br>
Le Xue<sup>1†</sup>, Mingfei Gao<sup>1</sup>, Chen Xing<sup>1</sup>, Roberto Mart´ın-Mart´ın<sup>1,2</sup>, Jiajun Wu<sup>3</sup>, Caiming Xiong<sup>1</sup>, Ran Xu<sup>1</sup>, Juan Carlos Niebles<sup>1</sup>, Silvio Savarese<sup>1</sup><br>
<sup>1</sup>Salesforce Research, Palo Alto, USA, <sup>2</sup>UT Austin, Texas, USA, <sup>3</sup>Stanford University, Stanford, USA

* [**ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding**](https://arxiv.org/pdf/2305.08275.pdf) [**Arxiv 2023**] <br>
Le Xue<sup>1†</sup>, Ning Yu<sup>1</sup>, Shu Zhang<sup>1</sup>, Junnan Li<sup>1</sup>, Roberto Martín-Martín<sup>3</sup>, Jiajun Wu<sup>2</sup>, Caiming Xiong<sup>1</sup>, Ran Xu<sup>1</sup>, Juan Carlos > Niebles<sup>1,2</sup>, Silvio Savarese<sup>1,2</sup><br>
<sup>1</sup>Salesforce AI, <sup>2</sup>Stanford University, <sup>3</sup>The University of Texas at Austin

* [**PLA: Language-Driven Open-Vocabulary 3D Scene Understanding**](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.pdf) [**CVPR 2023**] <br>
Runyu Ding<sup>1*</sup>, Jihan Yang<sup>1*</sup>, Chuhui Xue<sup>2</sup>, Wenqing Zhang<sup>2</sup>, Song Bai<sup>2†</sup>, Xiaojuan Qi<sup>1†</sup><br>
<sup>1</sup>The University of Hong Kong, <sup>2</sup>ByteDance

> ### 3D Dense Captioning
* [**End-to-End 3D Dense Captioning with Vote2Cap-DETR**](https://arxiv.org/pdf/2301.02508.pdf) [**CVPR 2023**] <br>
Sijin Chen<sup>1*</sup>, Hongyuan Zhu<sup>2</sup>, Xin Chen<sup>3</sup>, Yinjie Lei<sup>4</sup>, Tao Chen<sup>1†</sup>, Gang YU<sup>3</sup><br>
<sup>1</sup>Fudan University, <sup>2</sup>Institute for Infocomm Research, A<sup>*</sup>STAR, <sup>3</sup>Tencent PCG, <sup>4</sup>Sichuan University

> ### 3D Cross-Modal Retrieval
* [**RONO: Robust Discriminative Learning with Noisy Labels for 2D-3D Cross-Modal Retrieval**](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2023_paper.pdf) [**CVPR 2023**] <br>
Yanglin Feng<sup>1</sup>, Hongyuan Zhu<sup>2</sup>, Dezhong Peng<sup>1,3,4</sup>, Xi Peng1 Peng Hu<sup>1†</sup><br>
<sup>1</sup>College of Computer Science, Sichuan University, <sup>2</sup>Institute for Infocomm Research (I2R), A*STAR, <sup>3</sup>Sichuan Zhiqian Technology, <sup>4</sup>Chengdu Ruibei Yingte Information Technology

> ### 3D Visual QA
* [**3D Concept Learning and Reasoning from Multi-View Images**](https://openaccess.thecvf.com/content/CVPR2023/papers/Hong_3D_Concept_Learning_and_Reasoning_From_Multi-View_Images_CVPR_2023_paper.pdf) [**CVPR 2023**] <br>
Yining Hong<sup>1</sup>, Chunru Lin<sup>2</sup>, Yilun Du<sup>3</sup>, Zhenfang Chen<sup>5</sup>, Joshua B. Tenenbaum<sup>3</sup>, Chuang Gan<sup>4,5</sup><br>
<sup>1</sup>UCLA, <sup>2</sup>Shanghai Jiaotong University, <sup>3</sup>MIT CSAIL, <sup>4</sup>UMass Amherst, <sup>5</sup>MIT-IBM Watson AI Lab

> ### 3D Visual Grounding
* [**EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding**](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf) [**CVPR 2023**] <br>
Yanmin Wu<sup>1</sup>, Xinhua Cheng<sup>1</sup>, Renrui Zhang<sup>2,3</sup>, Zesen Cheng<sup>1</sup>, Jian Zhang<sup>1†</sup><br>
<sup>1</sup>Shenzhen Graduate School, Peking University, <sup>2</sup>The Chinese University of Hong Kong, <sup>3</sup>Shanghai AI Laboratory

